{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIMIZATION PROBLEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Introduction\n",
    "\n",
    "We have the following problem: We are looking for the optimal path from our home to our office.\n",
    "\n",
    "Consider the figure above: we start on the top left road and want to go on the bottom right road. If we reach this objective, we earn \\\\$10 . We can cross the top right bridge but it will cost us \\$5. We can not take the bottom left and center roads because they are closed.\n",
    "\n",
    "<img src=\"Optimization_Problem.jpg\" width=400>\n",
    "\n",
    "We are tired and distracted. If we decide to move to one direction, we have 70% probability that we actually move in that direction, and 30% probability that we walk to any of the other admissible directions.\n",
    "\n",
    "In this project, we will be considering both the deterministic and the stochastic problems.\n",
    "\n",
    "**Recall:** Here, deterministic setting means that our decision is not affected by random noise; stochastic setting means that the 70% rule (as described above) holds when we move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Set up the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up the notebook\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Define gamma and epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "epsilon = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART I - DETERMINISTIC VERSION OF THE PROBLEM\n",
    "\n",
    "First, we consider the deterministic version of the problem. By using the value iteration method, we compute the vector V(s) with s the state (taking values between 0 and 6). Then, by using such values, we compute the Q-matrix Q(s, a) with a the action (taking values between 0 and 3).\n",
    "\n",
    "**Notice:** \n",
    "- The output of this part is one vector and one matrix. \n",
    "- For actions, we use the convention 0 = up, 1 = right, 2 = down, 3 = left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. 1) Define the rewards for the deterministic version R_s_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = [\n",
    "    [0,0,0,0],\n",
    "    [0,-5,0,0],\n",
    "    [0,0,0,0],\n",
    "    [0,0,0,0],\n",
    "    [0,0,0,0],\n",
    "    [-5,0,10,0]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. 2) Define V_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_next = [\n",
    "    [6,1,3,6],\n",
    "    [6,2,4,0],\n",
    "    [6,6,5,1],\n",
    "    [0,4,6,6],\n",
    "    [1,5,6,3],\n",
    "    [2,6,6,4]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. 3) Bellman Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables\n",
    "\n",
    "V_ini = np.array([0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0., 10.,  0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write down the Bellman equations\n",
    "\n",
    "V = []\n",
    "\n",
    "for s in range(0,6):\n",
    "    L = []\n",
    "    for a in range(0,4):\n",
    "        y = R[s][a] + gamma*V_ini[ V_next[s][a] ]\n",
    "        L.append(y)\n",
    "    V.append(max(L))\n",
    "V.append(0)\n",
    "\n",
    "V = np.array(V)\n",
    "V    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. 4) Iteration Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V0(s) = 0 for each s in S\n",
    "\n",
    "V_0 = V_ini\n",
    "V_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0., 10.,  0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First iteration V_1(s)\n",
    "\n",
    "V_1 = V\n",
    "V_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector {V(s)} is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 7.29,  8.1 ,  9.  ,  8.1 ,  9.  , 10.  ,  0.  ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration\n",
    "\n",
    "u = abs(V_0 - V_1)\n",
    "\n",
    "while max(u) > epsilon:\n",
    "    V_0 = V_1\n",
    "   \n",
    "    V = []\n",
    "    for s in range(0,6):\n",
    "        L = []\n",
    "        for a in range(0,4):\n",
    "            y = R[s][a] + gamma*V_0[ V_next[s][a] ]\n",
    "            L.append(y)\n",
    "        V.append(max(L))\n",
    "    V.append(0)\n",
    "    V_1 = np.array(V)\n",
    "    u = abs(V_0 - V_1)\n",
    "\n",
    "V = V_1\n",
    "print('The vector {V(s)} is:')\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. 5) Compute the Q matrix with elements Q_s_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Q matrix {Q(s,a)} is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 7.29, 7.29, 0.0],\n",
       " [0.0, 3.0999999999999996, 8.1, 6.561],\n",
       " [0.0, 0.0, 9.0, 7.29],\n",
       " [6.561, 8.1, 0.0, 0.0],\n",
       " [7.29, 9.0, 0.0, 7.29],\n",
       " [3.0999999999999996, 0.0, 10.0, 8.1],\n",
       " [0, 0, 0, 0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = []\n",
    "\n",
    "for s in range(0,6):\n",
    "    L = []\n",
    "    for a in range(0,4):\n",
    "        y = R[s][a] + gamma * V[ V_next[s][a] ]\n",
    "        L.append(y)\n",
    "    Q.append(L)\n",
    "    \n",
    "Q.append([0,0,0,0])\n",
    "\n",
    "print('The Q matrix {Q(s,a)} is:')\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART II - STOCHASTIC VERSION OF THE PROBLEM\n",
    "\n",
    "We consider now the stochastic version of the problem. We adapt the code in part I to this setting. Again, the output is one vector and one matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. 1) Define the probabilities for the stochastic version P_s_a_s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [\n",
    "    [[0,0,0,0,0,0,0] , [0,0.7,0,0.3,0,0,0] , [0,0.3,0,0.7,0,0,0] , [0,0,0,0,0,0,0]],\n",
    "    [[0,0,0,0,0,0,0] , [0.15,0,0.7,0,0.15,0,0] , [0.15,0,0.15,0,0.7,0,0] , [0.7,0,0.15,0,0.15,0,0]],\n",
    "    [[0,0,0,0,0,0,0] , [0,0,0,0,0,0,0] , [0,0.3,0,0,0,0.7,0] , [0,0.7,0,0,0,0.3,0]],\n",
    "    [[0.7,0,0,0,0.30,0,0] , [0.3,0,0,0,0.7,0,0] , [0,0,0,0,0,0,0] , [0,0,0,0,0,0,0]],\n",
    "    [[0,0.7,0,0.15,0,0.15,0] , [0,0.15,0,0.15,0,0.7,0] , [0,0,0,0,0,0,0] , [0,0.15,0,0.7,0,0.15,0]],\n",
    "    [[0,0,0.7,0,0.15,0,0.15] , [0,0,0,0,0,0,0] , [0,0,0.15,0,0.15,0,0.7] , [0,0,0.15,0,0.7,0,0.15]]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. 2) Define the rewards for the deterministic version R_s_a_s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reward = [\n",
    "    [[0,0,0,0,0,0,0] , [0,0,0,0,0,0,0] , [0,0,0,0,0,0,0] , [0,0,0,0,0,0,0]],\n",
    "    [[0,0,0,0,0,0,0] , [0,0,-5,0,0,0,0] , [0,0,-5,0,0,0,0] , [0,0,-5,0,0,0,0]],\n",
    "    [[0,0,0,0,0,0,0] , [0,0,0,0,0,0,0] , [0,0,0,0,0,0,0] , [0,0,0,0,0,0,0]],\n",
    "    [[0,0,0,0,0,0,0] , [0,0,0,0,0,0,0] , [0,0,0,0,0,0,0] , [0,0,0,0,0,0,0]],\n",
    "    [[0,0,0,0,0,0,0] , [0,0,0,0,0,0,0] , [0,0,0,0,0,0,0] , [0,0,0,0,0,0,0]],\n",
    "    [[0,0,-5,0,0,0,10] , [0,0,0,0,0,0,0] , [0,0,-5,0,0,0,10] , [0,0,-5,0,0,0,10]]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. 3) Define V_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_next = [\n",
    "    [6,1,3,6],\n",
    "    [6,2,4,0],\n",
    "    [6,6,5,1],\n",
    "    [0,4,6,6],\n",
    "    [1,5,6,3],\n",
    "    [2,6,6,4]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. 4) Bellman Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 6.25, 0]\n"
     ]
    }
   ],
   "source": [
    "V_ini = [0,0,0,0,0,0,0,0]\n",
    "V = []\n",
    "\n",
    "for s in range(0,6):\n",
    "    L = []\n",
    "    for a in range(0,4):\n",
    "        u = 0\n",
    "        for ss in range(0,7):\n",
    "            u += P[s][a][ss] * ( Reward[s][a][ss] + gamma*V_ini[ V_next[s][a] ] )\n",
    "        L.append(u)\n",
    "    V.append(max(L))\n",
    "    \n",
    "V.append(0)\n",
    "\n",
    "print(V)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. 5) Iteration Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V0(s) = 0 for each s in S\n",
    "\n",
    "V_0 = np.array([0,0,0,0,0,0,0])\n",
    "V_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.  , 0.  , 0.  , 0.  , 6.25, 0.  ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First iteration V_1(s)\n",
    "\n",
    "V_1 = np.array(V)\n",
    "V_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector {V(s)} is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.55625, 4.3125 , 5.625  , 5.0625 , 5.625  , 6.25   , 0.     ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iteration\n",
    "\n",
    "u = abs(V_0 - V_1)\n",
    "\n",
    "while max(u) > epsilon:\n",
    "    V_0 = V_1\n",
    "    \n",
    "    V = []\n",
    "    for s in range(0,6):\n",
    "        L = []\n",
    "        for a in range(0,4):\n",
    "            y = 0\n",
    "            for ss in range(0,7):\n",
    "                y += P[s][a][ss] * ( Reward[s][a][ss] + gamma*V_0[ V_next[s][a] ] )\n",
    "            L.append(y)\n",
    "        V.append(max(L))\n",
    "    V.append(0)\n",
    "    \n",
    "    V_1 = np.array(V)\n",
    "    u = abs(V_0 - V_1)\n",
    "\n",
    "V = V_1\n",
    "print('The vector {V(s)} is:')\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. 6) Compute the Q matrix with elements Q_s_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 3.8812499999999996, 4.55625, 0.0],\n",
       " [0.0, 1.5625, 4.3125, 3.350625000000001],\n",
       " [0.0, 0.0, 5.625, 3.8812499999999996],\n",
       " [4.100625000000001, 5.0625, 0.0, 0.0],\n",
       " [3.8812499999999996, 5.625, 0.0, 4.55625],\n",
       " [3.0625, 0.0, 6.25, 5.8125],\n",
       " [0, 0, 0, 0]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = []\n",
    "\n",
    "for s in range(0,6):\n",
    "    L = []\n",
    "    for a in range(0,4):\n",
    "        y = 0\n",
    "        for ss in range(0,7):\n",
    "            y+= P[s][a][ss] * (Reward[s][a][ss] + gamma * V[ V_next[s][a] ])\n",
    "        L.append(y)\n",
    "    Q.append(L)\n",
    "    \n",
    "Q.append([0,0,0,0])\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# CONCLUSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V represents the optimal strategy to adopt to solve the problem. \n",
    "\n",
    "In the first case, the problem is deterministic. It means that the reward and V only depend on the current state s and on the action a since the destination is deterministic and then known. Therefore, it makes sense that the V and then Q in the first case are higher than the V and Q in the second case. Thus, the values in Q in the stochastic problem decrease. However, even if the values decrease, we can see that the maximum in each row stay in the same position: the optimal strategy is still the same for the two problems!\n",
    "\n",
    "Optimal strategy to go to Etcheverry Hall for the stochastic problem:\n",
    "\n",
    "- 0 – 3 – 4 – 5 – 6\n",
    "\n",
    "\n",
    "Finally, we can see that for the deterministic problem, there are two different optimal strategies since the probabilities are not taken into account:\n",
    "\n",
    "- 0 – 3 – 4 – 5 – 6\n",
    "- 0 – 1 – 4 – 5 – 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
